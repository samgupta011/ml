
# üéØ Principal Component Analysis (PCA) with Iris Dataset

This project demonstrates how to apply **Principal Component Analysis (PCA)** to reduce the dimensions of the **Iris dataset** and visualize the results. PCA is an unsupervised machine learning technique used for dimensionality reduction, which helps in visualizing high-dimensional data in a lower-dimensional space while preserving the variance.

---

## üì¶ Requirements

You can install the necessary libraries using pip:

```bash
pip install numpy pandas matplotlib seaborn scikit-learn
```

---

## üìÅ Line-by-Line Code Explanation

### 1. **Import Libraries**

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
```

- `numpy` ‚Üí for numerical operations and data manipulation.
- `pandas` ‚Üí for creating data structures and handling datasets.
- `matplotlib.pyplot` ‚Üí for plotting the PCA results.
- `seaborn` ‚Üí for advanced data visualization.
- `load_iris` ‚Üí loads the Iris dataset from `scikit-learn`.
- `StandardScaler` ‚Üí used for standardizing features (important for PCA).
- `PCA` ‚Üí the class that implements Principal Component Analysis.

---

### 2. **Load the Iris Dataset**

```python
iris = load_iris()
X = iris.data
y = iris.target
target_names = iris.target_names
```

- `load_iris()` loads the famous Iris dataset, which contains measurements of 150 iris flowers in 4 features (sepal length, sepal width, petal length, petal width) for 3 species.
- `X` contains the feature data (4 features).
- `y` contains the target (species classification: 0, 1, 2).
- `target_names` holds the species names (`setosa`, `versicolor`, `virginica`).

---

### 3. **Standardize the Data**

```python
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

- PCA is sensitive to the scale of the features. We standardize the data to have zero mean and unit variance for each feature, ensuring that the principal components are not dominated by features with larger ranges.

---

### 4. **Apply PCA**

```python
pca = PCA(n_components=2)  # Reduce to 2 components for visualization
X_pca = pca.fit_transform(X_scaled)
```

- `PCA(n_components=2)` creates an instance of the PCA class, specifying that we want to reduce the data to 2 principal components (for visualization purposes).
- `fit_transform()` performs the PCA transformation on the standardized data, reducing the dimensionality of `X_scaled` from 4 to 2.

---

### 5. **Create a DataFrame for PCA Components**

```python
df_pca = pd.DataFrame(data=X_pca, columns=["PC1", "PC2"])
df_pca["target"] = y
```

- Creates a new `DataFrame` to store the reduced features (`PC1` and `PC2`).
- Adds the target variable (`y`) to the DataFrame for easy visualization (species labels).

---

### 6. **Visualize the PCA Result**

```python
plt.figure(figsize=(8, 6))
sns.scatterplot(x="PC1", y="PC2", hue="target", palette="Set1", data=df_pca)
plt.title("PCA of Iris Dataset")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend(title="Target", labels=target_names)
plt.grid(True)
plt.show()
```

- Creates a scatter plot with `PC1` on the x-axis and `PC2` on the y-axis.
- The `hue="target"` adds color differentiation based on the species of the iris flower.
- `palette="Set1"` specifies the color palette for different species.
- The plot title, axis labels, and legend are added for clarity.
- `grid(True)` adds a grid for better readability of the plot.

---

### 7. **Print Explained Variance**

```python
print("Explained Variance Ratio by each component:")
print(pca.explained_variance_ratio_)
```

- The explained variance ratio tells you how much information (variance) each principal component holds relative to the total variance.
- Printing this value helps you understand the effectiveness of PCA in capturing the most important features of the data.

---

## üìå Summary

- **PCA** reduces the dimensionality of the dataset while retaining as much variance (information) as possible.
- **Iris Dataset** has 4 features; we reduced them to 2 components for easier visualization.
- **Standardization** ensures that PCA isn't biased by features with larger scales.
- The visualization shows how well PCA separates the 3 species.

---

## üöÄ How to Run

1. Install dependencies.
2. Copy the code into a Python file or Jupyter Notebook.
3. Run the script to see the PCA results and visualizations.

---

## üìä Results

- The plot shows the Iris dataset in two principal components.
- Each point in the plot represents a flower and is color-coded based on the species.

---

## üë©‚Äç‚öïÔ∏è Use Case

PCA can be useful in reducing the complexity of multi-dimensional datasets, improving the efficiency of machine learning models, and helping visualize complex relationships in the data. It's commonly used in fields like image compression, finance, and bioinformatics.
